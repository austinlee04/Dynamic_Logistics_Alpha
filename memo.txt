21.10.31
할 일 : (비용 계산) 구현
다음에 할 일 : sim 파일에서 hub_classification 명령어 수행하도록 해야 함.       ## done

21.11.03
할 일 :
- sim 파일에서 허브 상차 구현         ## done
- edge 추가하는 함수(env)           ## done
다음에 할 일 :
- env.traffic list out of range 에러 해결
- sim 완성

21.11.09
할 일 :
- V2 제작(sim, env, data)
다음에 할 일 :
- 운송 종료되었을 때 처리과정
- log 저장

21.11.11
할 일 :
- REINFORCE 학습 모듈 제작 시작함
다음에 할 일 :
- REINFORCE 학습 모듈 완성
- 훈련 시도

21.11.29
다음에 할 일 :
- train 과정 구현
(state = 허브 포화도)

21.12.10
다음에 할 일 :
- 도착한 set 대한 처리
(학습에 필요, 매우 중요함!!!)

21.12.11
다음에 할 일 :
- sim_V3에서 reward 계산파트 구현(in get_result())
- sim_V3 오류 조짐 있음...ㅠ

21.12.13
할 일 :
- 오류 고치기
- 학습 과정 및 결과 그래프 그리기
다음에 할 일 :
- 구간 통행량이 0으로 잡히는 경우가 존재함. 이 때문에 비용 구하는 과정에서 division by zero 에러 발생.
현재는 임시조치를 취했지만, 추후 고쳐야 함.
- 그 외에도 division by zero 에러가 sim_V3의 get_result 에서 비용 구하는 과정에서 자주 발생.
임시조치에 분모에 +1 하도록 함.

21.12.14
다음에 할 일 :
- catch those fucking errors!!!

21.12.28
Note :
- division by zero 에러가 에피소드 1개당 1~2개 꼴로 발생
- parcel log 가 제대로 기록되지 않음(도착시간 기록 오류)

21.12.21
Note :
- parcel log 기록 X 원인 찾음 : 오류가 아님. 허브 들어간 후 대기열에 있다가 프로그램 종료됨

21.12.22
할 일 :
- dist/t, cost/t 각각 log 저장
- 학습 진행(100에피소드, MTE=12000)
- 시뮬레이션 진행(action 1~8)
- save_log 수정

21.12.23
Note :
- MTE 달성한 후 진행 과정에서 len(parcel)이 감소하지 않는 현상. 허브 정체 의심됨

21.12.25
Note :
- 학습 과정에서 state 반영되지 않음. 다만, 과거의 state 를 key 별로 저장하는 것이 비효율적이므로 time 별로 state 계산할 수 있게
dict 에 각 허브의 대기열 길이를 저장

21.12.30
Note :
- Memory error 발생함. numpy, pandas 도입 시급함. 데이터를 비효율적으로 사용되거나, del 정상적으로 이루어지는지 확인 필요.
  log 저장이 비효율적으로 이루어지는 경햠 있음. write 후 del 하는 방향으로 수정 바람.
- time 당 생성되는 parcel 수 증가 필요. (학습 잘 이루어지지 않음)
- 검증 및 비교용 Data set 생성.

21.12.31
할 일 :
- 메모리 효율적으로 사용하고 연산속도 개선 위해 numpy, pandas, deque 으로 자료구조 변경 및 알고리즘 개선

22.01.03
Note :
- 보상 계산식을 바꿔야 함. 거리 * 통행량 / 시간
- 급하게 바꿀 수 있는 부분만 np.array 로 바꾸어 학습 다시 할 것. (episodes=100, MTE=400)